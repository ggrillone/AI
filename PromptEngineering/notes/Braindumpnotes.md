Outline topics

TODO
* Using the responses from the survey: https://docs.google.com/forms/d/1tnKQBfDoTCC6FxcmsVIUUQHbsrVdIvnPpvhLVZg9O_k/edit
* Weave in examples that help devs with specific areas they care about, e.g. playwright debugging

* Opening statement
    * AI is not as magical as some think, it’s not a silver bullet. It takes time to learn how to use effectively. Experiment and practice! Build good habits so it becomes second nature. It will be more work at first, but will get better.
* LLM settings
    * Briefly talk about for awareness, but don’t go too deep
        * Temperature
            * Controls how deterministic a model is
            * Low temp = more deterministic
            * High temp = more randomness / diverse output
        * Top P
            * Controls how deterministic a model is
            * Low = more factual
            * High = More creative
            * When using Top P, only the tokens in the Top P are considered for a response
                * Low Top P = selects response that the model is more confident in
                * High Top P = looks at more possible responses, including less likely ones
        * Do not use Temperature and Top P at the same time
        * Max length: control the max tokens generated by the model
        * Stop sequences: Constraints that when met, stop the model from generating a longer response
        * Frequency Penalty
            * Applies a penalty on the next token proportional to how many times that token already appeared in the response and prompt
                * Lower = repeated word more likely to appear
                * Higher = repeated word less likely to appear
        * Presence Penalty
            * Similar to Frequency Penalty, except the penalty is not proportional to number of times a token has been used, the penalty is consistent
            * Prevents the model from repeating phrases too often
            * Lower value = More focus
            * Higher value = More diverse/creative 
        * Do not use Frequency Penalty and Presence Penalty at the same time
    * You won’t really use these unless you are making API calls to ChatGPT etc…
    * Cursor and other AI IDE tools likely have this configured already
    * One setting worth calling out is “stop sequence” - a similar idea can be leveraged in prompts to be more explicit in defining when the model should stop so it does not generate more content than needed / reduce output token use
* Briefly talk about tokens and context window
    * TODO: add more information here
    * Input / output tokens
    * Context windows
        * https://docs.anthropic.com/en/docs/build-with-claude/context-windows
        * There’s a practical limit to how much information a model can consider at once = context window
        * Working memory
        * Your prompts + AI response + other information you provide —> context window
* Basics of prompts
    * You don’t need to use all of these elements for every prompt, it depends on the task.
    * Prompt anatomy
        * Prompt formatting
            * Structuring your prompts with formatting is beneficial because
                * Clarity: Clearly separate different parts of your prompt and ensure your prompt is well structured.
                * Accuracy: Reduce errors caused by the model misinterpreting parts of your prompt.
                * Flexibility: Easily find, add, remove, or modify parts of your prompt without rewriting everything.
                * Parseability: Makes it easier to extract specific parts of its response by post-processing.
            * Different ways to structure prompts
        * Organize your content clearly, allowing the model to infer structure without using explicit labels.
        * Simple labels
            * Output: Generate the content in raw .md format.
        * Header labels (using md syntax)
            * # Output Generate the content in raw .md format.
        * XML style
            * <output>   Generate the content in raw .md format. </output>
    * Best practices
        * Be consistent in naming
        * Properly nest
            * .e.g for md format use the correct number of “#” when nesting
        * Instruction: a specific task or instruction you want the model to perform
        * Context: external information or additional context that can steer the model to better responses
        * Input data: the input or question that we are interested to find a response for
        * Output format/indicator: the type or format of the output.
        * There can be more, these are just the most common
        * For more complex problems
            * Don’t have a conversation with the agent, correcting it as it makes mistakes
            * Do invest more thought into your prompt, context, and instructions. Provide a clear set of instructions for the agent to follow. If the response is wrong (parts or the whole response), think about why it’s wrong and what you can change, then start a new chat - correct your inputs.
        * Before engaging with AI, it’s crucial to have a clear understanding of your desired outcome.
            * Define Success Criteria: Be specific about what success looks like for your project. What are you trying to achieve with the AI’s help?
            * Clarify Your Vision: Know exactly what you want to accomplish before you start prompting the model. Vague requests can lead to unclear or suboptimal responses.
            * Analogy
                * Think about proposing a new web framework to your team. You wouldn’t simply say, “Let’s use this because it’s cool.”
                * Instead, you’d present clear evidence and reasoning—why this framework is a good fit, what problems it solves, and how it aligns with your team’s goals. Similarly, when working with AI, back up your request with specific objectives and reasoning.
    * Additional prompt elements I use:
        * Role and Objective: Defines who the AI should act as (e.g., "expert iOS developer," "senior React engineer") and states the primary task or goal. Sets the persona and main mission for the interaction.
        * Checklist: Requests a concise outline of key tasks or approach before proceeding. Used to ensure clear planning and task breakdown.
        * Validation: Requires verification that objectives are met after each step or completion. Often includes self-correction instructions if validation fails.
        * Stop conditions: Defines when to pause, stop, or seek clarification. Prevents over-execution and ensures proper handoff points.
        * Workflow instructions: Process-focused guidance that defines the sequence of operations, iteration patterns, and handover procedures between AI agents.
        * Planning and verification: Requests upfront planning with clear objectives and validation steps to ensure quality before implementation.
        * Error handling: Defines how to handle incomplete items, failures, or uncertainties. Includes escalation procedures and documentation requirements.
        * Constraints: Explicit limitations and boundaries, including technical restrictions, accessibility requirements, and forbidden approaches.
    * When creating a prompt think about…
        * https://anthropic.skilljar.com/ai-fluency-framework-foundations
        * Delegation: what parts you want to handle vs. the AI
            * Problem awareness
                * It is important to have a clear understanding of the problem you are trying to solve
                * What does success look like
                * What kind of thinking and work is needed
                * Without a clear understanding of your goals and the work involved, you will struggle with AI
                * Clearly define your goals and understand what work is needed before bringing in AI
            * Platform awareness
                * It is important to have a clear understanding of what the model can and can’t provide
                * Tools it can use
                * Is the model built for speed or depth
                * Is the model better for accuracy or creativity
                * What model works best for each task?
                * Where does AI add value
            * Task delegation
                * Make thoughtful decisions to strategically divide the work between you and the model
                * Break down complex pieces into smaller pieces
                * When does it make sense for a human to be involved
                * What can usefully be automated?
                * Where does it make more sense to use AI for augmentation
        * Description: How you can clearly communicate your task
            * One does not simply write a prompt (LotR reference)
            * Explain tasks, ask questions, provide context, and guide the interaction
            * Product description
                * Clearly describe what you want
                * Explain every relevant detail, you don’t want the AI to make assumptions - don’t make AI guess
                * What format do you want, who’s the audience, what are other constraints
            * Process description
                * Guide how the AI approaches the task
                * Sometimes “how” is more important than “why” - how should the AI get to the end solution
                    * Self-reflection
                    * Cite documentation
                    * Consider dependency X
                    * Consider making reusable code
                * What would be your process/method for solving this problem yourself
                * Is there data you need the model to draw from
                * Is there workflow you want it to follow?
            * Performance description
                * Define how you want the AI to behave
                * What kind of thinking partner do you need
                    * Do you need a factual response? Or are you in a creative thinking process?
                    * Do you want the AI to challenge you?
                    * Do you want an educator/mentor role or do you just want the answer
            * Review for ambiguity
            * If you handed off this description to a coworker with no context, would they be able to complete the task?
            * If you can’t explain it, you don’t understand it yet
        * Discernment: Once you have run your prompt, you need to critically evaluate
            * With great power comes great responsibility - Spider Man
            * The quality control system for AI collaboration
            * A log or mind map can help evaluate quality and help you refine your process
            * Product discernment
                * Did the output accomplish the task / meet the requirements
                    * Is it factually accurate
                    * Is it coherent and well structured
                    * Does it add value
            * Process discernment
                * Ensure the user and model’s thinking are aligned
                * Create an AI-human dynamic that enables the best thinking for both the human and AI
                    * Think how YOU as a person learn/interact best - think of our recent PI behavioral assessment
                    * Use analogies to explain complex information
                    * Be skeptical
                    * Wait until I have finish my thought process before providing criticism
                    * Use humor in your response 
                * Also asses how the AI thought process about the problem that led to the final solution/answer
                    * Are there logical errors
                    * Were there unnecessary or irrelevant steps
                    * Did it get stuck in a loop 
            * Performance discernment
                * The behavior of the AI - how well the AI is interacting with the user
                * Is the communication style correct
                * Is the information provided clear and provide meaning
                * Does the model respond to additional feedback and steering
            * If something is wrong, adjust the prompt to correct it, iterate
        * Diligence: interact with AI responsibly
            * The ethical and safety aspects of your interactions
            * What are the implications of working with the AI
            * Who might be effected by what is created
            * Can I provide this data to AI
            * Creation Diligence: Being thoughtful about which AI systems you choose and how you work with them
            * Transparency Diligence: Being open about AI's role in your work
            * Deployment Diligence: Taking ownership for AI-assisted outputs you share with others
            * You are responsible, not the AI
        * Loops
            * Delegation <-> Diligence loop
                * Handles big-picture strategic and ethical decision-making about AI use
                * Delegation and Diligence each informs and shapes the other — the loop runs both ways
                * This loop develops clear rationales for user choices and can help articulate why your approach aligns with goals and values
                * Accountability and transparency enhance rather than limit creative possibilities with AI
            * Description <-> Discernment loop
                * Transforms AI interaction from commands to conversations that use context to build cognitive environments
                * Product, Process, and Performance operate as different lenses for understanding the same collaborative process
                * Teaching this loop means helping students move beyond automation to genuine augmentation
                * Successful cognitive environments include shared vocabulary, established interaction patterns, and mechanisms for building on previous exchanges
            * The two loops work as nested systems—strategic decisions create the container that tactical interactions fill
    * Effective prompting techniques
        * https://anthropic.skilljar.com/ai-fluency-framework-foundations
        * Foundational tips
            * Provide context: be specific about what you want, why you want it, and relevant background information
            * Offer examples
                * Few-shot/n-shot/multi-shot prompting
                * Provide good and bad examples
            * Specify output constraints
                * Text, JSON, markdown, code, etc…
                * Summary or detailed
                * Cite sources
                * How should the information be organized
                * Audience
                    * A non-programmer should be able to understand this
                    * A developer with no context of the project should be able to understand this
                    * An AI agent should be able to follow the instructions and implement the code changes
            * Break complex tasks to steps and ask it to think
                * Chain of thought prompting (CoT)
                * Can be vague like (more open to creativity)
                    * Think step-by-step
                    * Think through the problem carefully
                * Or provide more detailed thinking steps (you have a process that works, you want to guide the model to mimic your method)
                    * Review the data, find the top 3 rated items, generate a summary about the products
            * Define the role, style, and tone
                * You are a senior frontend developer
                * You are a quality engineer that specializes in playwright
                * You are a mentor
                * You are a critical thinker that challenges ideas and assumptions
            * Ask the AI to help improve your prompt
                * Anthropic (Seems you need an Anthropic account): https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver 
                * OpenAI: https://platform.openai.com/chat/edit?optimize=true
            * Effective prompting is experimental and iterative
            * When a conversation goes off track, sometimes starting with a fresh chat is better than trying to correct the model
            * Patterns that work well
                * Start with a clear task overview statement
                * Include output format specifications and examples
                * Detailing specific constraints or requirements
                * Providing rich and relevant background information
            * Common mistakes to avoid
                * Assuming the model can read your mind or think like you do
                * Overloading a single prompt or conversation with multiple unrelated tasks
                * Being vague about defining what success looks like
                * Not providing feedback on previous responses
* Vague (open-ended) vs. specific prompts
    * Not all prompts need to be detailed, and not all should be vague. Both have value — the key is being intentional about which style you choose based on your goal.
    * Be thoughtful about how specific or open-ended your prompt is — vague prompts are great for discovery, specific prompts are great for precision.
    * When crafting prompts, you can think of them on a spectrum—from vague to specific. How specific your prompt is will influence how the model responds.
        * Think back to the LLM settings temperature
            * Note this does not actually change this setting, but the behavior is similar
    * Feeling stuck or not sure where to start? Beginning with a vague prompt could be a good place to start
        * Use a vague AI prompt to throw out a wide net
        * e.g. What are best practices for validating an email address?
            * The AI might point you to a custom regular expression, provide a few existing libraries that provide this type of validation, and edge cases to be aware of
    * Think of vague prompts as
        * Higher temperature configuration
        * Feel open-ended and exploratory.
        * You want the AI to think more freely when you give a vague prompt
        * Useful for
            * Brainstorming ideas (e.g. “Suggest UI improvements for a signup form”)
            * You want to generate multiple UI ideas or code patterns.
            * Exploring alternative solutions or perspectives
            * You’re looking for conceptual or architectural options.
            * Getting unstuck
        * Benefit: Allow the model to explore a wider solution space, which can lead to novel ideas you might not have considered.
        * Downside: Can produce off-topic or overly general results.
    * Specific prompts
        * Lower temperature
        * You need explicit guard rails for the AI to follow
        * Contain clear instructions and constraints.
        * Useful for
            * Getting structured or repeatable outputs (e.g. “Write a React functional component that renders a signup form with email and password inputs”)
            * Following strict design patterns or business logic
            * You want help optimizing or debugging something specific.
            * You’re aiming to save time on boilerplate or patterns.
        * Benefit: More control and consistency.
        * Downside: Might miss creative or alternative approaches
    * Combining vague + specific
        * You can outline a specific end goal without providing strict guard rails on the solution
            * e.g. I need to overlay help text on top of a specific HTML element. The text needs to be displayed to the right of the target HTML element. The text needs to remain next to the HTML element as the user scrolls. The solution needs to work with multiple screen sizes and also adapt to the screen if the user resizes the browser.
                * We avoid telling the agent to use getBoundingClientRects or use CSS absolute positioning. This way we don’t constrain the agents thinking.
    * TODO:
        * Provide example to demo
            * Use case for when to use vague prompt
                * Write vague + specific prompt and compare
            * Use case for when to use specific prompt
                * Write vague + specific prompt and compare
* Tips for prompts
    * Personal thoughts…
        * Don’t assume the AI with think like you
            * The more I use AI the more I make my prompt like my thought process - mirror that in your prompts
                * An excerpt from this Anthropic article on how they built their “research” feature: https://www.anthropic.com/engineering/multi-agent-research-system 
                * “Our prompting strategy focuses on instilling good heuristics rather than rigid rules. We studied how skilled humans approach research tasks and encoded these strategies in our prompts—strategies like decomposing difficult questions into smaller tasks, carefully evaluating the quality of sources, adjusting search approaches based on new information, and recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel).”
            * When approaching a problem that you want to solve with AI, think about how you would solve it without AI, e.g. how would you approach it?
                * Write it out / map out your internal thought process (which is just a good practice to do anyway)
                    * First I need to understand class X
                    * Then I need to know what else calls this function
                    * This function takes inputs x, y, z and the caller expects this response object
    * Start simple
    * Be intentional with vague (open-ended) vs. specific prompts
        * Are you in the early stages of solving a problem and exploring and want more creative thinking?
            * Prompt content should be higher level and not too specific otherwise the model’s thinking process will be narrow.
        * Do you clearly know what you want?
            * Provide specific examples, inputs, outputs, constraints, etc…
    * Be thoughtful
        * Don’t just prompt “Here’s the error, fix it”
        * Better:
            * I am encountering the following error: … It happens when I click on a button and try to apply text color styles. The function is being called with x, y, z
    * Avoid ambiguity – If your prompt can be interpreted in multiple ways, the AI may make incorrect assumptions. Be clear and specific to guide the AI toward the response you want.
        * It’s okay to be vague in certain situations
        * It’s not okay to be ambiguous 
    * Experiment + iterate
        * Learn from what doesn’t work
    * Expand the AI “thought” drop-down in chat
    * Steer the agent (Cursor)
        * In Cursor you can send additional messages while the model is thinking if you need to add additional context or notice the model is going in the wrong direction
            * Message will be queued then sent once the agent completes the current step
            * There’s also a button where you can force push the message now
            * TODO: come up a good live example to demonstrate this
    * Break tasks down into smaller tasks - models perform better when they are focused
    * When a prompt doesn’t work try to understand what went wrong
    * When a prompt doesn’t work ask questions like
1. Clarity & Context
        * Was there enough context?
        * Was there too much context?
            * What % context did the prompt use?
                * In Cursor you can see this in the chat window
        * Was my wording too vague?
        * Did I clearly state the goal or output format?
        * Did I specify what not to include?
        * Was I mixing too many tasks into one prompt?
2. Assumptions & Mental Models
        * What assumptions did I make that the AI didn’t?
        * Did I assume domain knowledge the AI might not prioritize?
        * Did I assume the AI "knows" my intent without stating it?
3. Structure & Scope
        * Was the prompt too open-ended or too constrained?
        * Was the scope of the request too large for a single response?
        * Did I include step-by-step instructions or just ask for an outcome?
        * Was the level of detail appropriate for the task?
4. Iteration & Output Quality
        * Were more changes made than expected?
        * What parts of the response were correct? What weren’t?
        * Did the AI hallucinate (i.e., make things up)? Why might that be?
        * Did it follow all instructions or miss part of the task?
        * Was the tone, structure, or format off?
5. Prompt Formulation
        * Could I break this into multiple prompts?
        * Did I try using examples (few-shot prompting) to guide the response?
        * Could I rephrase this more like a user story or task description?
        * Did I use too much "fluff" or not enough clear directives?
* Best practices
    * https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices#example-creating-an-analytics-dashboard 
    * Models respond well to clear, explicit instructions. Being specific about your desired output can help enhance results.
    * Providing context or motivation behind your instructions, such as explaining to the model why such behavior is important, can help it better understand your goals and deliver more targeted responses.
    * Provide additional context such as
        * Inputs and outputs (end goal)
        * What the task will be used for
        * The audience that the output is intended for
        * Other relevant information for the particular tasks
    * Ensure that your examples align with the behaviors you want to encourage and minimize behaviors you want to avoid.
    * Control the format of responses
    * Encourage self-reflection and verification
        * Take time to write this out
        * Imagine you were going to perform the task with AI, what would you do?
* Prompting techniques
    * Example to use for zero-shot, few-shot, and chain of thought
        * TODO: think of a coding example 
    * TODO
        * Think of generic simple prompt
            * Start as zero-shot example
            * Then show turning it into few-shot
            * Then show turning to CoT
                * Also zero-shot+CoT
                * Also few-shot+CoT
            * Continue building on same example with meta prompting, ToT, etc..
        * Hands on exercise
            * Need to come up with a problem
            * Have someone write a zero-shot prompt
            * Then turn it into a few-shot prompt etc…
    * Role prompting
        * https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts
        * Give clear instructions on the base behavior or role of the model
        * Benefits
            * Improves accuracy in complex tasks
            * Defines the tone and communication style
            * Improves the focus of the model
        * Examples:
            * You are a senior frontend developer
            * You maintain a mentorship mentality
            * You are an AI expert
            * You are a senior quality engineer specializing in playwright
    * zero-shot: instructing the model to perform a task without providing any examples
        * https://arxiv.org/abs/2109.01652
        * When the task is simple, low-stakes, or something the model is already well-trained on (like summarization, translation, or fact lookup)
        * Rely on model training knowledge and pattern recognition to get an answer
    * few-shot (multi-shot / n-shot): provide examples for the model to reference
        * https://arxiv.org/abs/2005.14165 
        * When tasks are ambiguous and you want the AI to learn from examples
        * The model needs to be steered
        * Teach the model by demonstration instead of only by instruction.
        * Why use examples
            * Improve accuracy, reduces misinterpretation
            * Enforces consistency
            * Improves how the model performs with complex tasks
        * Tips for examples
            * Keep examples relevant to your use case
            * Provide a diverse set of examples (3-5), include edge cases
            * Can also include examples of what you want it NOT to do
                * e.g. Do not write useless tests like:
                    * jest.spyOn(myClass, ‘funcName’).mockReturnValue(true)
                    * expect(myClass.funcName()).toBe(true)
            * Be explicit in your prompt text that they are examples
    * Chain of thought (CoT): involves asking the model to break down the steps it takes to reach an answer. By guiding the model to articulate its reasoning, you help it arrive at a more accurate, logical, and comprehensive response.
        * https://arxiv.org/abs/2201.11903
        * When tasks need logical steps, math, planning, or complex reasoning
        * Use it when you prompt the model, explicitly instruct it to "think through" the problem or provide a step-by-step explanation. This encourages the model to generate responses that are more thoughtful and well-reasoned, rather than just providing a direct answer.
        * When faced with complex tasks like research, analysis, or problem-solving, CoT can dramatically improve the model’s performance.
        * CoT is not necessary for every task
        * Example 1:
            * This large code file has grown to an unmanageable state and I need help refactoring.
            * Think step by step and briefly show your reasoning before each change
        * Example 2: even something this simple can be beneficial
            * Think step-by-step
        * Example 3: telling the model to explain its reasoning before making a decision and guide it to using CoT
            * Before selecting a tool, explain why it is being used
    * Few-shot + CoT is good for more complex tasks
    * Meta prompting: 
        * https://arxiv.org/abs/2311.11482
            * https://github.com/meta-prompting/meta-prompting 
        * Provide a behavior/structural template that tells the model how to think and respond
        * Allows you to control the behavior and style of the response
        * Example: 
            * This large code file has grown to an unmanageable state and I need help refactoring.
            * Review the code file, identify refactoring opportunities, plan the changes, review and validate, output the response.
    * CoT vs. meta prompting: CoT encourages the model to show reasoning steps, but doesn’t define the structure of those steps (Meta prompting).
    * Self consistency
        * https://arxiv.org/abs/2203.11171 
        * Involves prompting the model multiple times with the same set of instructions and same CoT content but varying factors like temperature and top-p or using a different model
            * Review the multiple reasoning paths that the model takes
            * Compare the final answers across the reasoning paths
            * Instead of trusting one answer, trust the majority consensus
            * Useful for: there is one correct answer - you want to understand different reasoning paths
            * Not useful: exploring multiple possible answers
    * Prompt chaining
        * A method to improve the performance and reliability of a model is by breaking a prompt into subtasks and using the output of a previous prompt as input to the following prompt
        * This can be as simple as planning in 1 prompt and feeding that output into an execution phase
        * Or it can be used to solve large complex problems where the whole task in split into subtasks, each prompt output feeds to the next subtask
            * Multi-step analysis: See the legal and business examples below.
            * Content creation pipelines: Research → Outline → Draft → Edit → Format.
            * Data processing: Extract → Transform → Analyze → Visualize.
            * Decision-making: Gather info → List options → Analyze each → Recommend.
            * Verification loops: Generate content → Review → Refine → Re-review.
        * Because of context limits, it is highly recommended to have the agent store output in a file like an .md file
            * It is good practice to tell the agent that your audience is an AI agent (that may not have the full context), this will steer it to include enough detail to be consumed by another AI agent
            * Context won’t be tossed
            * You run out of context, now it is now easier to switch to a new chat window
        * Example
            * Part 1: You are a helpful assistant. Review the following web page and search for all car models. Output the content in the form of “Car model: …, Car type: …, Top speed: …”
            * Part 2: Using the provided list of car information, give me the top 3 fastest cars. <Car info output from last prompt>
    * Tree of Thoughts (ToT)
        * https://arxiv.org/abs/2305.10601
        * https://arxiv.org/abs/2305.08291
        * https://github.com/dave1010/tree-of-thought-prompting
        * Similar to chain of though and prompt chaining - but prompting the model to self-evaluate which creates new branches of thought
            * Instead of generating reasoning in a single linear chain, Tree of Thoughts enables the model to explore multiple branches of reasoning—evaluating and expanding different possible lines of thought, like a tree.
            * Encourage the model to look forward and backward
            * Having the model explore multiple reasoning paths instead of just 1
        * Analogy: Learning a new UI library like React
            * You have a problem, you want to know if React can solve that problem
            * You begin by reading what React can do in general, what components are, etc..
            * Then you start focusing on specific parts of React like hooks and props.
                * You consider if there are better alternatives within React
                * Evaluate each path, fail fast and move away from pieces that won’t help solve your problem
                * Backtrack if something doesn’t work out
                * Each of these branches of thought help answer small questions (branches)
            * When you have an understanding of each piece, you now have the whole exploration tree and can determine if React can solve your particular problem
    * Automatic Reasoning and Tool-use (ART)
        * https://arxiv.org/abs/2303.09014 
        * As you interact with the model, it reasons about the task and automatically chooses the right tool to use
        * For example you are working in a coding project and you ask it to find all code that calls “myFunc()”, the model would select the tool to use the “grep” command to search
    * Automatic Prompt Engineer (APE)
        * https://arxiv.org/abs/2211.01910
        * https://arxiv.org/abs/2205.11916 
        * APE is a tool to automate prompt creation for LLMs.
        * It generates prompts based on predefined task requirements or specifications, rather than requiring humans to craft them manually.
        * APE aims to improve the efficiency of the prompt engineering process, making it accessible to those who lack deep expertise in LLMs.
    * Active Prompt
        * https://arxiv.org/pdf/2302.12246
        * Refers to an approach where the model actively searches for the best possible prompts through a process that involves exploration and feedback. Rather than manually crafting and fine-tuning prompts, the model autonomously refines prompts to achieve better performance on a specific task.
        * The model is trained to explore different prompt formulations (i.e., ways of structuring a prompt) and to evaluate which prompts lead to the most optimal responses.
            * This is done through a feedback loop where the model adjusts its prompts based on the effectiveness of previous ones.
    * Directional Stimulus Prompting
        * https://arxiv.org/abs/2302.11520
        * Uses a smaller policy model to generate directional stimuli (textual hints) that are prepended/appended to the input and fed into the large LLM. This steers the LLM toward more desirable outputs.
        * Training of the policy model:
            * First with supervised fine-tuning (SFT) on pseudo-stimulus pairs (e.g., extracting keywords from references).
            * Then optionally with reinforcement learning (RL) (e.g., PPO), where rewards come from the quality of the LLM’s output given the stimulus.
        * Example: For summarization, the policy model outputs keywords (directional stimulus) that highlight important points from the source. Those keywords, when included in the prompt, guide the LLM to cover them in its summary.
    * Program-Aided Language Models (PAL)
        * https://arxiv.org/abs/2211.10435 
        * The idea of offloading the solution to computer program.
        * Instead of using reasoning, it generates a program (or is provided with one) to perform a task and the execution of the program becomes the reasoning step.
            * For example instead of having a model reason to figure out a math problem, give it a python script to execute
        * Codex is an AI coding agent by OpenAI that translates natural language into code, helping developers write, review, and ship code faster. It is a specialized version of OpenAI's language model, capable of understanding and generating programming code, and can assist with various software development tasks, such as editing files, navigating codebases, and running terminal commands.
            * https://github.com/openai/codex 
    * ReAct Prompting
        * https://arxiv.org/abs/2210.03629
            * “A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, :  & Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky, 1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992).”
        * The idea of using the model to generate the reasoning (Re) and task actions (Act) for a prompt
            * A synergy between acting and reasoning
            * Example: You are trying to figure out what state management in your React application
                * Perform research on the top 3 recommended state management systems for a React application. Identify the problems that each system solves and any shortcomings it may have. Cite real world examples.  Use tools to cross-check claims across at least 3 reputable sources before finalizing summaries. Before outputting a response, validate and fact check all information. If something is not represented factually, adjust. Continue until each of the 3 recommendations is summarized in 1-2 paragraphs each.
        * As a prompt is executing, this enables the model to adjust its reasoning and actions as needed
            * Because the model sees the history of Thoughts, Actions, and Observations, it can track its own progress through the task.
            * A programming example: “Now I have written this function, I must go write unit tests. I see this project uses React testing library, I will use that… I don’t know how to query for a particular DOM selector, let me look that up in the documentation.”
        * The framework allows the model to use tools to retrieve external information to produce better results
        * Chain of thought is good for reasoning structure, but can hallucinate factual information. By leveraging ReAct we can reduce the hallucinations.
        * Prompt recipe:
            * You are an agent that alternates short Thoughts and Actions. Use Actions to gather evidence, and Observations will appear after actions.
            * 
            * Q: [question]
            * 
            * Thought: [<brief reasoning plan…]
            * Action: search[...]
            * Observation: [system will append results]
            * 
            * ...continue until...
            * Action: finish[Final answer]
        * Cons:
            * Can eat up a lot of tokens, especially if guard rails/stop conditions are not included
            * ReAct still depends on the quality of the external tools and observations — bad inputs = bad outputs.
    * Reflexion
        * https://arxiv.org/pdf/2303.11366 
            * “This is akin to how humans iteratively learn to accomplish complex tasks in a few-shot manner – by reflecting on their previous failures in order to form an improved plan of attack for the next attempt.”
            * “We develop a modular formulation for Reflexion, utilizing three distinct models: an Actor, denoted as Ma, which generates text and actions; an Evaluator model, represented by Me, that scores the outputs produced by Ma; and a Self-Reflection model, denoted as Msr, which generates verbal reinforcement cues to assist the Actor in self-improvement.”
        * Reflexion is a method that enables a language model to improve its performance through structured self-reflection across multiple episodes. Instead of relying on fine-tuning or reinforcement learning, the model is guided to evaluate its own outputs, identify mistakes, and adapt its future behavior using memory or prompt-based strategies—without changing its underlying parameters.
    * Multimodal Chain of Thought (CoT) Prompting
        * https://arxiv.org/abs/2302.00923
        * Enhancing CoT by providing multiple sources of information. Instead of just providing context through text, provide an image.
        * This framework is demonstrated through model training, but in practice it can also be mimicked in multimodal prompting setups (e.g. tools like Cursor IDE), where a text prompt and an attached image are both provided as context.
        * The idea focuses on images as the second source of information, but could include other types of data like audio.
    * Toolformer
        * https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/ 
        * The idea of training a model to effectively use tools - it can decide when to make tools calls and which arguments to pass
        * More of a model-side idea.
        * Toolformer is a training/finetuning method. A model doesn’t come with it “by default.” If you want a model to use tools the Toolformer way, you need to explicitly augment training data with tool calls and finetune it.
        * Toolformer vs. Automatic Reasoning and Tool-use (ART)
            * With ART the user is giving an example/instructing when to use the tool. And with Toolformer it is pretrained on this and requires no explicit user input on when to all a tool
            * So when I'm using the Cursor IDE and I give it a task to complete - it might need to search for a function call. it will use "grep" for this and it does it automatically. is this an example of ART or Toolformer?
                * When you give Cursor a task like “find where this function is called,” it often decides on the fly to run a shell command like grep to get that info, then continues reasoning with the result.
                * That looks more like ART than Toolformer because:
                    * The base model is frozen (it hasn’t been finetuned with “always use grep here”).
                    * The inference-time reasoning framework decides when/where to invoke external tools.
                    * The system provides demonstrations / scaffolding (e.g. “when you need to search, use grep”), and the model plugs that tool call into its reasoning chain.
                * Toolformer, by contrast, would have had to pretrain or finetune on many examples where a grep-like API call was inserted and proven useful. After that, it would just “know” to call grep when asked something like “where is this function defined?” — without an external wrapper prompting it.
    * Personal experience
        * AILog.md
            * Use for larger tasks
            * Have the AI keep a log of the progress to make it easier to switch chats and continue
                * Yes in Cursor you can refer to past chats - but it condenses/summarizes the chats and we don’t have control of that. Maybe it’s my lack of trust, but I assume if we rely on that then there is a chance an important piece of information could be lost
        * Planning file
            * Use for larger and/or more complex tasks - helps with clearly outlining the bigger picture and you can then work through the inner details
            * Start with planning and context gathering which is written to a .md file
            * Once complete, have an agent sequentially work through the plan to implement the changes
        * Execution checklist
            * As an agent is working through your planning file, have it create a checklist file for reference
            * Sometimes unexpected problems occur and you might need to dive deeper into a particular problem
                * Can break out into a separate chat and then resume in your original chat
        * Stubbing code using code comments
            * Have found this useful for using AI for unit tests
            * Use code comments to write in natural language what I want to test, specific assertions, what should be mocked, etc…
            * Have AI implement the tests, following the code comments
* Reducing hallucinations
    * https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations#example-analyzing-a-merger-and-acquisition-report
    * https://openai.com/index/why-language-models-hallucinate 
    * When a model generates text that is factually incorrect or inconsistent
    * Model’s don’t like to tell you “I don’t know”
    * Give explicit instructions that it is okay to say “I don’t know”
    * Or you can tell it to reference and cite a particular article/documentation
    * Tips
        * Chain-of-thought verification: Ask the model to explain its reasoning step-by-step before giving a final answer. This can reveal faulty logic or assumptions.
        * Best-of-N verificiation: Run the model through the same prompt multiple times and compare the outputs. Inconsistencies across outputs could indicate hallucinations.
        * Iterative refinement: Use the model’s outputs as inputs for follow-up prompts, asking it to verify or expand on previous statements. This can catch and correct inconsistencies.
        * External knowledge restriction: Explicitly instruct the model to only use information from provided documents and not its general knowledge.
    * Examples
        * When encountering an unknown, question or consideration make note of it in the output and request the user to review and resolve the item. Make no assumptions.
        * Verify in the React documentation that the syntax is valid in React 16.7
* System prompts
    * https://www.lesswrong.com/posts/HjHqxzn3rnH7T45hp/do-you-even-have-a-system-prompt-psa-repo 
    * TODO: add more information
* Prompt templates and variables
    * https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables
    * Use syntax such as “{{FeatureSwitchName}}” to act as a placeholder (variable) in a prompt file to make it reusable
    * Cursor does not directly support prompt templates, but through rules or prompt instructions we might be able to make use of this
        * TODO: test this out
        * https://forum.cursor.com/t/built-in-prompt-templates/92100/2 
* Golden test set
    * TODO: add more information here
* Use AI to help refine your prompts
    * https://platform.openai.com/chat/edit?optimize=true
* Choosing a model
    * Token costs across models: https://token-calculator.net/   
    * Anthropic models: https://docs.anthropic.com/en/docs/about-claude/models/choosing-a-model 
    * OpenAI models: https://platform.openai.com/docs/models
    * Google models: https://cloud.google.com/vertex-ai/generative-ai/docs/models 
* Tools available to models
    * Anthropic model tools: https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview 
    * OpenAI model tools: https://platform.openai.com/docs/guides/tools 
    * Google model tools: https://ai.google.dev/gemini-api/docs/live-tools 

Recommend checking these out
* https://www.promptingguide.ai/ 
* https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview
* https://anthropic.skilljar.com/ai-fluency-framework-foundations 

Additional resources:
* https://cookbook.openai.com/ 
* https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide
* https://platform.openai.com/docs/guides/prompting 
* https://docs.anthropic.com/en/docs/build-with-claude/context-windows 
* https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations#example-analyzing-a-merger-and-acquisition-report 
* https://openai.com/index/why-language-models-hallucinate 
* https://anthropic.skilljar.com/
* https://github.com/anthropics/courses 
* https://github.com/anthropics/anthropic-cookbook 
* https://arxiv.org/abs/2109.01652
* https://arxiv.org/abs/2005.14165
* https://arxiv.o